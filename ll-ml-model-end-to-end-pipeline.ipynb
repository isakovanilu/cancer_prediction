{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68215e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249f2e8-cf9e-424d-b697-a2d6e48edb3f",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "This notebook outlines a complete SageMaker pipeline for building, training, and registering a machine learning model using the Scikit-learn framework and SageMaker's Linear Learner algorithm. Hereâ€™s a summary of its components and flow:\n",
    "\n",
    "#### Initialization\n",
    "Initializes a SageMaker session and the role required to execute the pipeline.\n",
    "Defines pipeline parameters (S3 data paths, instance type) for flexibility in configurations.\n",
    "\n",
    "#### Data Processing\n",
    "\n",
    "Uses SKLearnProcessor to preprocess data (e.g., test-data.csv). A ProcessingStep is defined to execute the preprocessing with outputs for training and validation datasets.\n",
    "Data is processed and stored in S3 for further use.\n",
    "\n",
    "#### Model Training\n",
    "\n",
    "Defines a TrainingStep to train a binary classification model using the SageMaker Linear Learner algorithm.\n",
    "The training step takes in processed training and validation data from the previous step.\n",
    "\n",
    "#### Model Registration\n",
    "\n",
    "Registers the trained model in the SageMaker model registry using RegisterModel. This makes the model available for deployment and further evaluations.\n",
    "\n",
    "#### Pipeline Definition\n",
    "\n",
    "Combines the steps (data processing, training, and model registration) into a single Pipeline. The pipeline is created or updated and executed using SageMaker's pipeline capabilities.\n",
    "\n",
    "#### Model Deployment\n",
    "\n",
    "Uses the SageMaker client to find and retrieve the latest model package from the model registry and deploys to the Endpoints.\n",
    "\n",
    "#### Conclusion\n",
    "The pipeline automates the entire machine learning workflow from preprocessing, training, model registration, and preparation for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15242de",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea4172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.model import ModelPackage\n",
    "\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "pipeline_name = \"linear-linear\"  # SageMaker Pipeline name\n",
    "\n",
    "# Define bucket, prefix, role\n",
    "\n",
    "\n",
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb374080-2550-49cf-b9ea-b3c9f1245128",
   "metadata": {},
   "source": [
    "# Define a Processing Step for Feature Engineering <a class=\"anchor\" id=\"training\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b127ed-f635-4c92-8b5b-86a1270c898a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Define input and output paths based on environment variables\n",
    "input_data_path = os.path.join('/opt/ml/processing/input', 'test-data.csv')  # S3 input gets mounted here\n",
    "\n",
    "output_train_path = os.path.join('/opt/ml/processing/output/train', 'train.csv')\n",
    "output_validation_path = os.path.join('/opt/ml/processing/output/validation', 'validation.csv')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()        \n",
    "        \n",
    "    # Encode categorical features\n",
    "    le_gender = LabelEncoder()\n",
    "    le_cancer_type = LabelEncoder()\n",
    "    data['outcome'] = data['outcome'].apply(lambda x: 1 if x == 'survived' else 0)\n",
    "    data['gender'] = le_gender.fit_transform(data['gender'])\n",
    "    data['cancer_type'] = le_cancer_type.fit_transform(data['cancer_type'])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Split data into features and labels if needed\n",
    "    if 'outcome' in data.columns:\n",
    "        X = data.drop('outcome', axis=1)\n",
    "        y = data['outcome']\n",
    "    else:\n",
    "        X, y = data, None\n",
    "\n",
    "    return X, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    print(\"Reading input data from:\", input_data_path)\n",
    "    data = pd.read_csv(input_data_path)\n",
    "\n",
    "    # Preprocess the data\n",
    "    print(\"Preprocessing data...\")\n",
    "    X, y = preprocess_data(data)\n",
    "    \n",
    "    # Scale the numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Convert to CSV format and save locally\n",
    "    # Ensure the label is the first column in the dataset\n",
    "    train_data = pd.concat([y_train.reset_index(drop=True), pd.DataFrame(X_train)], axis=1)\n",
    "    print('train_data', train_data.head(2))\n",
    "    validation_data = pd.concat([y_test.reset_index(drop=True), pd.DataFrame(X_test)], axis=1)\n",
    "    print('validation_data', validation_data.head(2))\n",
    "    \n",
    "    print(\"Saving train and validation data\", output_train_path, ' + ', output_validation_path)\n",
    "\n",
    "    # Save the datasets as CSV files without headers\n",
    "    train_data.to_csv(output_train_path, index=False, header=False)\n",
    "    validation_data.to_csv(output_validation_path, index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1f224-cee6-4c64-bb24-ebe3e16b6bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker import ModelPackage\n",
    "\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "# Define Pipeline Parameters (optional for flexibility)\n",
    "train_data_param = ParameterString(\n",
    "    name='TrainData',\n",
    "    default_value=f's3://{bucket}/{prefix}/input/test-data.csv'\n",
    ")\n",
    "output_prefix_param = ParameterString(\n",
    "    name='OutputPrefix',\n",
    "    default_value=f's3://{bucket}/{prefix}/processed/'\n",
    ")\n",
    "instance_type_param = ParameterString(\n",
    "    name='InstanceType',\n",
    "    default_value='ml.m5.xlarge'\n",
    ")\n",
    "\n",
    "# Define the Scikit-learn processor for data preprocessing\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='1.0-1',  # Adjust based on your sklearn version\n",
    "    role=role,\n",
    "    instance_type=instance_type_param,\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define the ProcessingStep\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"DataProcessingStep\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=train_data_param,  # S3 path to test-data.csv\n",
    "            destination='/opt/ml/processing/input'  # Container path\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output/train/',  # Container path for train.csv\n",
    "            destination=f's3://{bucket}/{prefix}/processed/train/',  # S3 path\n",
    "            output_name='train_data'  # This is the name of the output\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output/validation/',  # Container path for validation.csv\n",
    "            destination=f's3://{bucket}/{prefix}/processed/validation/',  # S3 path\n",
    "            output_name='validation_data'  # This is the name of the validation output\n",
    "        )\n",
    "    ],\n",
    "    code=\"preprocessing.py\"  # Ensure this script is in your working directory\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Linear Learner Estimator\n",
    "linear_estimator = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve('linear-learner', boto3.Session().region_name),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size=30,  # in GB\n",
    "    max_run=3600,  # in seconds\n",
    "    output_path=f's3://{bucket}/{prefix}/output/',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "\n",
    "# Set hyperparameters for the Linear Learner\n",
    "linear_estimator.set_hyperparameters(\n",
    "    predictor_type='binary_classifier',\n",
    "    mini_batch_size=10\n",
    ")\n",
    "\n",
    "# Define TrainingStep using the correct outputs from ProcessingStep\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainLinearLearnerModel\",\n",
    "    estimator=linear_estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,  # Correct reference to 'train_data'\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"validation_data\"].S3Output.S3Uri,  # Correct reference to 'validation_data'\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# (Optional) Define Model Registration Step\n",
    "model_register_step = RegisterModel(\n",
    "    name=\"RegisterLinearLearnerModel\",\n",
    "    estimator=linear_estimator,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=MODEL_PACKAGE_GROUP_NAME\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Pipeline with steps in sequence\n",
    "pipeline = Pipeline(\n",
    "    name=\"LinearLearnerPipeline\",\n",
    "    steps=[processing_step, training_step, model_register_step],\n",
    "    parameters=[\n",
    "        train_data_param,\n",
    "        output_prefix_param,\n",
    "        instance_type_param\n",
    "    ],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "\n",
    "# Create or update the pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline_execution = pipeline.start()\n",
    "\n",
    "print(f\"Pipeline execution started with execution ARN: {pipeline_execution.arn\")\n",
    "\n",
    "# Wait for the pipeline execution to complete before deploying the endpoint\n",
    "pipeline_execution.wait()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06ea74-39be-477d-9294-9a670904068b",
   "metadata": {},
   "source": [
    "# Deploy the latest model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66a956-15ce-42dc-be02-d1b54db47156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize SageMaker client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# List model packages in the group and sort by creation time to get the latest version\n",
    "response = sm_client.list_model_packages(\n",
    "    ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=1\n",
    ")\n",
    "\n",
    "# Get the latest ModelPackageArn\n",
    "latest_model_package_arn = response['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "\n",
    "print(f\"Latest Model Package ARN: {latest_model_package_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d449d-c89a-4ddc-ab30-ddaeab6f9120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a ModelPackage object from the ModelPackageArn\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_model_package_arn,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "version_number = latest_model_package_arn.split('/')[-1]\n",
    "endpoint_name = f\"linear-learner-endpoint-{version_number}\"  # Choose your desired endpoint name\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # Adjust the instance type as needed\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "# Print the deployed endpoint name\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a76dea-e672-4374-ab8d-5a701a9ea58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
